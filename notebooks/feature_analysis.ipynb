{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis with Yellowbrick\n",
    "\n",
    "Feature analysis visualizers are designed to visualize instances in data space in order to detect features or targets that might impact downstream fitting. Because ML operates on high-dimensional data sets (usually several dozen!), the visualizers focus on aggregation, optimization, and other techniques to give overviews of the data. It is our intent that the steering process will allow the data scientist to zoom and filter and explore the relationships between their instances and between dimensions.\n",
    "\n",
    "**Note: If you haven't already downloaded the data, check out the instructions in the notebook called `get_the_data.ipynb` first!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yellowbrick as yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for separability\n",
    "\n",
    "### Radviz\n",
    "\n",
    "`RadViz` is a multivariate data visualization algorithm that plots each feature dimension uniformly around the circumference of a circle then plots points on the interior of the circle such that the point normalizes its values on the axes from the center to each arc. This mechanism allows as many dimensions as will easily fit on a circle, greatly expanding the dimensionality of the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.datasets import load_occupancy\n",
    "from yellowbrick.features import RadViz\n",
    "\n",
    "# Load the classification data set\n",
    "X, y = load_occupancy()\n",
    "\n",
    "# Specify the target classes\n",
    "classes = [\"unoccupied\", \"occupied\"]\n",
    "\n",
    "# Instantiate the visualizer\n",
    "visualizer = RadViz(classes=classes, size=(1080, 720))\n",
    "\n",
    "visualizer.fit(X, y)           # Fit the data to the visualizer\n",
    "visualizer.transform(X)        # Transform the data\n",
    "visualizer.poof()              # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists use this method to detect separability between classes. \n",
    "\n",
    "Is there an opportunity to learn from the feature set or is there just too much noise?\n",
    "\n",
    "### Parallel Coordinates\n",
    "\n",
    "Parallel coordinates is multi-dimensional feature visualization technique where the vertical axis is duplicated horizontally for each feature. Instances are displayed as a single line segment drawn from each vertical axes to the location representing their value for that feature. This allows many dimensions to be visualized at once; in fact given infinite horizontal space (e.g. a scrolling window), technically an infinite number of dimensions can be displayed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.features import ParallelCoordinates\n",
    "\n",
    "# Load data set (don't need to do again, included for completeness)\n",
    "X, y = load_occupancy()\n",
    "\n",
    "# Specify the features of interest and the classes of the target\n",
    "features = [\n",
    "    \"temperature\", \"relative humidity\", \"light\", \"CO2\", \"humidity\"\n",
    "]\n",
    "classes = [\"unoccupied\", \"occupied\"]\n",
    "\n",
    "# Instantiate the visualizer\n",
    "visualizer = ParallelCoordinates(\n",
    "    classes=classes, features=features, sample=0.05, \n",
    "    shuffle=True, size=(1080, 720)\n",
    ")\n",
    "\n",
    "# Fit and transform the data to the visualizer\n",
    "visualizer.fit_transform(X, y)\n",
    "\n",
    "# Finalize the title and axes then display the visualization\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists use this method to detect clusters of instances that have similar classes, and to note features that have high variance or different distributions.\n",
    "\n",
    "By inspecting the visualization closely, we can see that the combination of transparency and overlap gives us the sense of groups of similar instances, sometimes referred to as \"braids\". If there are distinct braids of different classes, it suggests that there is enough separability that a classification algorithm might be able to discern between each class.\n",
    "\n",
    "However, as we inspect this class, we can see that the domain of each feature may make the visualization hard to interpret. In the above visualization, the domain of the `light` feature is from in `[0, 1600]`, far larger than the range of temperature in `[50, 96]`. To solve this problem, each feature should be scaled or normalized so they are approximately in the same domain.\n",
    "\n",
    "Normalization techniques can be directly applied to the visualizer without pre-transforming the data (though you could also do this) by using the `normalize` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the visualizer\n",
    "visualizer = ParallelCoordinates(\n",
    "    classes=classes, features=features,\n",
    "    normalize='standard', # This time we'll specify a normalizer\n",
    "    sample=0.05, shuffle=True, size=(1080, 720)\n",
    ")\n",
    "\n",
    "# Fit the visualizer and display it\n",
    "visualizer.fit_transform(X, y)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using `minmax`, `minabs`, `standard`, `l1`, or `l2` normalization above to change perspectives!\n",
    "\n",
    "## Look for correlations\n",
    "\n",
    "### Feature Importances\n",
    "The feature engineering process involves selecting the minimum required features to produce a valid model because the more features a model contains, the more complex it is (and the more sparse the data), therefore the more sensitive the model is to errors due to variance. A common approach to eliminating features is to describe their relative importance to a model, then eliminate weak features or combinations of features and re-evalute to see if the model fairs better during cross-validation.\n",
    "\n",
    "Many model forms describe the underlying impact of features relative to each other. In scikit-learn, Decision Tree models and ensembles of trees such as Random Forest, Gradient Boosting, and Ada Boost provide a `feature_importances_` attribute when fitted. The Yellowbrick `FeatureImportances` visualizer utilizes this attribute to rank and plot relative importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from yellowbrick.features import FeatureImportances\n",
    "\n",
    "# Load data set (don't need to do again, included for completeness)\n",
    "X, y = load_occupancy()\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "viz = FeatureImportances(model, size=(1080, 720))\n",
    "viz.fit(X, y)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the features ranked according to the explained variance each feature contributes to the model. In this case the features are plotted against their *relative importance*, that is the percent importance of the most important feature. The visualizer also contains `features_` and `feature_importances_` attributes to get the ranked numeric values.\n",
    "\n",
    "For models that do not support a `feature_importances_` attribute, the `FeatureImportances` visualizer will also draw a bar plot for the `coef_` attribute that many linear models provide.\n",
    "\n",
    "When using a model with a `coef_` attribute, it is better to set `relative=False` to draw the true magnitude of the coefficient (which may be negative). We can also specify our own set of labels if the dataset does not have column names or to print better titles.\n",
    "\n",
    "### Rank 2D\n",
    "\n",
    "A two-dimensional ranking of features utilizes a ranking algorithm that takes into account pairs of features at a time (e.g. joint plot analysis). The pairs of features are then ranked by score and visualized using the lower left triangle of a feature co-occurence matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.datasets import load_credit\n",
    "from yellowbrick.features import Rank2D\n",
    "\n",
    "# Load the credit dataset\n",
    "X, y = load_credit()\n",
    "\n",
    "# Instantiate the visualizer with the Pearson ranking algorithm\n",
    "visualizer = Rank2D(algorithm='pearson', size=(1080, 720))\n",
    "\n",
    "visualizer.fit(X, y)\n",
    "visualizer.transform(X)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `Rank2D` visualizer utilizes the Pearson correlation score to detect colinear relationships.\n",
    "\n",
    "Alternatively, we can utilize the covariance ranking algorithm, which attempts to compute the mean value of the product of deviations of variates from their respective means. Covariance loosely attempts to detect a colinear relationship between features. Try substituting `algorithm='covariance'` above to see the difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the distribution\n",
    "\n",
    "\n",
    "### PCA Projection\n",
    "\n",
    "The PCA Decomposition visualizer utilizes principal component analysis to decompose high dimensional data into two or three dimensions so that each instance can be plotted in a scatter plot. The use of PCA means that the projected dataset can be analyzed along axes of principal variation and can be interpreted to determine if spherical distance metrics can be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.features.pca import PCADecomposition\n",
    "\n",
    "# Load data set (don't need to do again, included for completeness)\n",
    "X, y = load_credit()\n",
    "\n",
    "# Create a list of colors to assign to points in the plot\n",
    "colors = np.array(['r' if yi else 'b' for yi in y])\n",
    "\n",
    "visualizer = PCADecomposition(\n",
    "    scale=True, color=colors, size=(1080, 720)\n",
    ")\n",
    "visualizer.fit_transform(X, y)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA projection can also be plotted in three dimensions to attempt to visualize more principal components and get a better sense of the distribution in high dimensions. Try substituting in `visualizer = PCADecomposition(scale=True, color=colors, proj_dim=3)` above to see!\n",
    "\n",
    "### Manifold\n",
    "\n",
    "The `Manifold` visualizer provides high dimensional visualization using [manifold learning](https://scikit-learn.org/stable/modules/manifold.html) to embed instances described by many dimensions into 2, thus allowing the creation of a scatter plot that shows latent structures in data. Unlike decomposition methods such as PCA and SVD, manifolds generally use nearest-neighbors approaches to embedding, allowing them to capture non-linear structures that would be otherwise lost. The projections that are produced can then be analyzed for noise or separability to determine if it is possible to create a decision space in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.datasets import load_concrete\n",
    "from yellowbrick.features.manifold import Manifold\n",
    "\n",
    "# Load the concrete data set\n",
    "X, y = load_concrete()\n",
    "\n",
    "visualizer = Manifold(\n",
    "    manifold='isomap', target='continuous', size=(1080, 720)\n",
    ")\n",
    "visualizer.fit_transform(X,y)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Neighbor Embedding\n",
    "\n",
    "One very popular method for visualizing document similarity is to use t-distributed stochastic neighbor embedding, t-SNE. Scikit-learn implements this decomposition method as the `sklearn.manifold.TSNE` transformer. By decomposing high-dimensional document vectors into 2 dimensions using probability distributions from both the original dimensionality and the decomposed dimensionality, t-SNE is able to effectively cluster similar documents. By decomposing to 2 or 3 dimensions, the documents can be visualized with a scatter plot.\n",
    "\n",
    "Unfortunately, TSNE is very expensive, so typically a simpler decomposition method such as SVD or PCA is applied ahead of time. The `TSNEVisualizer` creates an inner transformer pipeline that applies such a decomposition first (SVD with 50 components by default), then performs the t-SNE embedding. The visualizer then plots the scatter plot, coloring by cluster or by class, or neither if a structural analysis is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.text import TSNEVisualizer\n",
    "from yellowbrick.datasets import load_hobbies\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the hobbies corpus (don't need to do again, included for completeness)\n",
    "corpus = load_hobbies() \n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "docs = tfidf.fit_transform(corpus.data)\n",
    "labels = corpus.target\n",
    "\n",
    "# Create the visualizer and draw the vectors\n",
    "tsne = TSNEVisualizer(size=(1080, 720))\n",
    "tsne.fit(docs, labels)\n",
    "tsne.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Frequency\n",
    "\n",
    "One method for visualizing the frequency of tokens within and across corpora is frequency distribution. A frequency distribution tells us the frequency of each vocabulary item in the text. In general, it could count any kind of observable event. It is a distribution because it tells us how the total number of word tokens in the text are distributed across the vocabulary items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the hobbies corpus\n",
    "corpus = load_hobbies()\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "docs = vectorizer.fit_transform(corpus.data)\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "visualizer = FreqDistVisualizer(features=features, size=(1080, 720))\n",
    "visualizer.fit(docs)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about Yellowbrick's feature visualization tools [here](http://www.scikit-yb.org/en/latest/api/features/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
